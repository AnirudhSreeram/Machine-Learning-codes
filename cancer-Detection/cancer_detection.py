# -*- coding: utf-8 -*-
"""cancer_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kW1BE2jDJl23ujFiv1p1FQN7BNJHmI8R
"""

import numpy
import torch
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, DataLoader
from torch.autograd import Variable
from network import Net


dataset = pd.read_csv('cancer_data.csv')
print(dataset)
print(dataset.shape)

dataset['diagnosis'] = dataset['diagnosis'].astype('category')
encode_map = {
    'M': 1,
    'B': 0
}

dataset['diagnosis'].replace(encode_map, inplace=True)
print(dataset)
X = dataset.drop('diagnosis',axis=1).astype(float)
Y = dataset['diagnosis']
print(X.shape)
print(Y.shape)
print(X)
print(Y)
X = dataset.iloc[:, 1:31].values
Y = dataset.iloc[:, 31].values
print(X.shape)
print(Y.shape)

# split the training data
classes = ('M', 'B')
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)
print(X_train.shape)
print(Y_train)
data = (X_train,Y_train)

### code to generate the data loaders
## train data
class trainData(Dataset):
    
    def __init__(self, X_data, y_data):
        self.X_data = X_data
        self.y_data = y_data
        
    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]
        
    def __len__ (self):
        return len(self.X_data)

train_data = trainData(torch.FloatTensor(X_train), 
                       torch.FloatTensor(Y_train))
## test data    
class testData(Dataset):
    
    def __init__(self, X_data):
        self.X_data = X_data
        
    def __getitem__(self, index):
        return self.X_data[index]
        
    def __len__ (self):
        return len(self.X_data)
    
test_data = testData(torch.FloatTensor(X_test))




EPOCHS = 150
BATCH_SIZE = 128
LEARNING_RATE = 0.001
wd = 0.1
net = Net()
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=0.9)
#optimizer = optim.Adam(net.parameters(), LEARNING_RATE, weight_decay=wd)
pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)
print(pytorch_total_params)

#set the loaders
train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)
test_loader  = DataLoader(dataset=test_data, batch_size=1)


## function for accuracy calculation
def binary_acc(y_pred, y_test):
    y_pred_tag = torch.round(torch.sigmoid(y_pred))
    y_test = torch.round(y_pred)
    correct_results_sum = (y_pred_tag == y_test).sum().float()
    acc = correct_results_sum/y_test.shape[0]
    acc = acc * 100
    return acc

### setting the model to train
net.train()
acc_1 = []
def train() :    
  for e in range(1, EPOCHS+1):
      epoch_loss = 0
      epoch_acc = 0
      for i, data in enumerate(train_loader,0):
          X_batch, y_batch = data
          optimizer.zero_grad()
          y_pred = net(Variable(X_batch.float()))
          y_batch = torch.unsqueeze(y_batch, 1) 
          loss = criterion(y_pred,Variable(y_batch.float())) 
          acc = binary_acc(y_pred, y_batch)
          loss.backward()
          optimizer.step()
          epoch_loss += loss.item()
          epoch_acc += acc.item()
      print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')
      acc_1.append(epoch_acc/len(train_loader))      

train()

plt.plot(acc_1, 'r--')
plt.ylabel('CV Accuracy')
plt.xlabel('epoch')
plt.show()

