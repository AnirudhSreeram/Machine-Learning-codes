# -*- coding: utf-8 -*-
"""MNIST(hand_written_digits).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Yn9Y3RZVDhg0dNvcONHWqPTCCzMAcKD
"""

#importing the necessary packages
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

#configuring the device
#device = torch.device('cuda' if torch.cuda.is_avaliable() else 'cpu')

#importing the MNIST data
train_data = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)
test_data = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)

train_loader = torch.utils.data.DataLoader(train_data,batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_data,batch_size)

examples = iter(train_loader)
sample, labels  = examples.next()
print(sample.shape)
print(labels.shape)

for i in range(6):
  plt.subplot(2,3,i+1)
  plt.imshow(sample[i][0], cmap='gray')
plt.show()

#Defining hyper parameters for tuning the models
input_size = 784
hidden_size = 100
number_classes = 10
num_epochs = 10
batch_size =128
lr = 0.001

# class for the network 1
class Net1(nn.Module):
  def __init__(self, input_size,hidden_size,number_classes):
    super(Net1, self).__init__()
    self.l1 = nn.Linear(input_size, hidden_size)
    self.l2 = nn.Linear(hidden_size, 50)
    self.l3 = nn.Linear(50, 50)
    self.l4 = nn.Linear(50, number_classes)
    self.relu = nn.ReLU()

  def forward(self, x):
    x = self.relu(self.l1(x))
    x = self.relu(self.l2(x))
    x = self.relu(self.l3(x))
    x = self.l4(x)
    return x

# class for the network 1
class Net2(nn.Module):
  def __init__(self, input_size,hidden_size,number_classes):
    super(Net2, self).__init__()
    self.l1 = nn.Conv2d(1, 8, kernel_size=8)
    self.l2 = nn.Conv2d(8, 8, kernel_size=8)
    self.l3 = nn.Conv2d(8, 8, kernel_size=7)
    self.l4 = nn.Conv2d(8, number_classes, kernel_size=8)

    #self.l4 = nn.Linear(8*10*10, number_classes)
    self.relu = nn.ReLU()

  def forward(self, x):
    #print(x.shape)
    x = self.relu(self.l1(x))
    x = self.relu(self.l2(x))
    x = self.relu(self.l3(x))
    #print(x.shape)
    #x = x.reshape(-1,16*10*10)
    #print(x.shape)
    x = self.l4(x)
    return x

### Initialize the loss and optimizer along with the network
net = Net2(input_size,hidden_size,number_classes)
pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)
print(pytorch_total_params)
criterion = nn.CrossEntropyLoss()
optim = torch.optim.Adam(net.parameters(), lr)

## test
  def test():
    with torch.no_grad():
      n_corr = 0
      n_samp = 0
      for i, data in enumerate(test_loader,0):
        input, label = data
        #input = input.reshape(-1,input.shape[1]*input.shape[2]*input.shape[3])
        #  forward pass
        output = net(input).reshape(-1,10*1*1)
        _, pred = torch.max(output,1)
        n_samp += label.shape[0]
        n_corr += (pred == label).sum().item()
    acc = 100.0 * n_corr/n_samp
    return acc

def train_test():    
    l =[]
    acc = []
    ## training loop
    total_step = len(train_loader)
    for epochs in range(num_epochs):
      for i , data in enumerate(train_loader,0):
        input, label = data
        #input = input.reshape(-1,input.shape[1]*input.shape[2]*input.shape[3])
        # forward pass
        output = net(input).reshape(-1,10*1*1)
        loss = criterion(output, label.detach().long())
        optim.zero_grad()
        loss.backward()
        optim.step()
        if ((i+1) %469 == 0 ):
          print(f' epoch {epochs + 1}/{num_epochs}, step {i+1}/{total_step}, loss = {loss.item():.4f}')
          l.append(loss)          
          acc.append(test())
    return l, acc

l , acc = train_test()
exp = "/home/anirudh"
torch.save(net.state_dict(),exp + '/dnn_MNIST.model')
plt.plot(l, 'r--')
plt.ylabel('CV loss')
plt.xlabel('epoch')
plt.show()

plt.plot(acc, 'r--')
plt.ylabel('CV Accuracy')
plt.xlabel('epoch')
plt.show()

